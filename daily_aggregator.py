#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ—¥æ¬¡é›†è¨ˆãƒ—ãƒ­ã‚°ãƒ©ãƒ 
Google Driveä¸Šã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å½“æ—¥ã®VCãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±ã‚’é›†è¨ˆã—ã€
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®çµ±è¨ˆæƒ…å ±ã‚’Google Sheetsã«æ›¸ãè¾¼ã‚€
"""

import os
import sys
import json
import base64
import argparse
from datetime import datetime, timedelta, date
from typing import Dict, List, Optional, Any
from collections import defaultdict
import io
from loguru import logger

# Google Drive/Sheets APIé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
from googleapiclient.errors import HttpError

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿
from dotenv import load_dotenv
load_dotenv()

# config.pyã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿
from config import EnvConfig, Environment

# loguruã®è¨­å®š
logger.remove()  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’å‰Šé™¤
logger.add(sys.stderr, level="INFO", format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {name}.py | def: {function} | {message}")  # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã¨é–¢æ•°åä»˜ãï¼‰

# logsãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
os.makedirs("logs", exist_ok=True)  # logsãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆï¼ˆæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰

# ç¾åœ¨ã®æ—¥ä»˜ã‚’å–å¾—ï¼ˆYYYYMMDDå½¢å¼ï¼‰
current_date = datetime.now().strftime("%Y%m%d")  # æ—¥ä»˜å–å¾—

# å‡¦ç†åˆ¥ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
# 1. ãƒ¡ã‚¤ãƒ³å‡¦ç†ã®ãƒ­ã‚°
logger.add(f"logs/daily_aggregator_{current_date}.log",
          rotation="10 MB",
          retention="7 days",
          level="INFO",
          encoding="utf-8",
          format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {name}.py | def: {function} | {message}",
          filter=lambda record: "get_csv" not in record["function"] and "aggregate" not in record["function"])  # ãƒ¡ã‚¤ãƒ³å‡¦ç†ãƒ­ã‚°

# 2. CSVãƒ•ã‚¡ã‚¤ãƒ«å–å¾—å‡¦ç†ã®ãƒ­ã‚°
logger.add(f"logs/csv_fetch_{current_date}.log",
          rotation="10 MB",
          retention="7 days",
          level="DEBUG",
          encoding="utf-8",
          format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {name}.py | def: {function} | {message}",
          filter=lambda record: "get_csv" in record["function"] or "read_csv" in record["function"])  # CSVå–å¾—ãƒ­ã‚°

# 3. é›†è¨ˆå‡¦ç†ã®ãƒ­ã‚°
logger.add(f"logs/aggregation_{current_date}.log",
          rotation="10 MB",
          retention="7 days",
          level="INFO",
          encoding="utf-8",
          format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {name}.py | def: {function} | {message}",
          filter=lambda record: "aggregate" in record["function"] or "write" in record["function"] or "update" in record["function"])  # é›†è¨ˆå‡¦ç†ãƒ­ã‚°

# 4. ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ï¼ˆå…¨ã¦ã®ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²ï¼‰
logger.add(f"logs/error_{current_date}.log",
          rotation="10 MB",
          retention="30 days",
          level="ERROR",
          encoding="utf-8",
          format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {name}.py | def: {function} | {message}")  # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°


class DailyAggregator:
    """æ—¥æ¬¡é›†è¨ˆå‡¦ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, target_date: Optional[date] = None, env: Environment = Environment.PRD):
        """
        åˆæœŸåŒ–

        Args:
            target_date: é›†è¨ˆå¯¾è±¡æ—¥ï¼ˆNoneã®å ´åˆã¯ä»Šæ—¥ï¼‰
            env: å®Ÿè¡Œç’°å¢ƒ
        """
        self.target_date = target_date or date.today()  # é›†è¨ˆå¯¾è±¡æ—¥
        self.env = env  # å®Ÿè¡Œç’°å¢ƒ
        self.drive_service = None  # Google Drive APIã‚µãƒ¼ãƒ“ã‚¹
        self.sheets_service = None  # Google Sheets APIã‚µãƒ¼ãƒ“ã‚¹
        self.credentials = None  # èªè¨¼æƒ…å ±

        # config.pyã‹ã‚‰è¨­å®šã‚’å–å¾—
        sheets_config = EnvConfig.get_google_sheets_config(env)  # Google Sheetsè¨­å®šå–å¾—
        drive_config = EnvConfig.get_google_drive_config(env)  # Google Driveè¨­å®šå–å¾—
        discord_config = EnvConfig.get_discord_config(env)  # Discordè¨­å®šå–å¾—

        self.sheet_name = sheets_config.get('sheet_name', 'VC_Tracker_Database')  # Sheetså
        self.folder_path = drive_config.get('folder_path', 'discord_mokumoku_tracker/csv')  # ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹
        self.allowed_vc_ids = discord_config.get('channel_ids', [])  # å¯¾è±¡VCãƒãƒ£ãƒ³ãƒãƒ«ID

        # åˆæœŸåŒ–å‡¦ç†
        self._initialize_services()


    def _initialize_services(self):
        """Google API ã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–"""
        try:
            # èªè¨¼æƒ…å ±ã®å–å¾—
            self.credentials = self._get_credentials()

            # Drive APIã‚µãƒ¼ãƒ“ã‚¹ã®æ§‹ç¯‰
            self.drive_service = build('drive', 'v3', credentials=self.credentials)
            logger.info("ğŸ“ Google Driveã¸ã®æ¥ç¶šãŒå®Œäº†ã—ã¾ã—ãŸ")  # åˆæœŸåŒ–æˆåŠŸãƒ­ã‚°

            # Sheets APIã‚µãƒ¼ãƒ“ã‚¹ã®æ§‹ç¯‰
            self.sheets_service = build('sheets', 'v4', credentials=self.credentials)
            logger.info("ğŸ“Š Google Sheetsã¸ã®æ¥ç¶šãŒå®Œäº†ã—ã¾ã—ãŸ")  # åˆæœŸåŒ–æˆåŠŸãƒ­ã‚°

        except Exception as e:
            logger.error(f"âš ï¸ ã‚µãƒ¼ãƒ“ã‚¹ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")  # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
            raise

    def _get_credentials(self):
        """èªè¨¼æƒ…å ±ã‚’å–å¾—"""
        # config.pyã‹ã‚‰èªè¨¼æƒ…å ±ã‚’å–å¾—
        sheets_config = EnvConfig.get_google_sheets_config(self.env)  # Google Sheetsè¨­å®šå–å¾—
        service_account_json_base64 = sheets_config.get('service_account_json_base64')  # Base64èªè¨¼æƒ…å ±
        service_account_file = sheets_config.get('service_account_json')  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

        if service_account_json_base64:
            # Base64ãƒ‡ã‚³ãƒ¼ãƒ‰
            service_account_json = base64.b64decode(service_account_json_base64).decode('utf-8')
            service_account_info = json.loads(service_account_json)
            logger.info("ğŸ” ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èªè¨¼æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸï¼ˆBase64å½¢å¼ï¼‰")  # Base64èªè¨¼ä½¿ç”¨ãƒ­ã‚°
        else:
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰èª­ã¿è¾¼ã¿
            if not os.path.exists(service_account_file):
                raise FileNotFoundError(f"Service account file not found: {service_account_file}")
            with open(service_account_file, 'r') as f:
                service_account_info = json.load(f)
            logger.info(f"ğŸ” èªè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {service_account_file}")  # ãƒ•ã‚¡ã‚¤ãƒ«èªè¨¼ä½¿ç”¨ãƒ­ã‚°

        # ã‚¹ã‚³ãƒ¼ãƒ—è¨­å®š
        scopes = [
            'https://www.googleapis.com/auth/drive',
            'https://www.googleapis.com/auth/spreadsheets'
        ]

        # èªè¨¼æƒ…å ±ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        return service_account.Credentials.from_service_account_info(
            service_account_info,
            scopes=scopes
        )

    def get_csv_files_from_drive(self) -> List[Dict[str, str]]:
        """
        Google Driveã‹ã‚‰CSVãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—

        Returns:
            CSVãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®ãƒªã‚¹ãƒˆ [{id, name}, ...]
        """
        try:
            logger.info(f"ğŸ” CSVãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œç´¢ã‚’é–‹å§‹ã—ã¾ã™")  # æ¤œç´¢é–‹å§‹ãƒ­ã‚°
            logger.info(f"ğŸ“ æ¤œç´¢ãƒ‘ã‚¹: {self.folder_path}")  # æ¤œç´¢ãƒ‘ã‚¹è¡¨ç¤º

            # ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‹ã‚‰ãƒ•ã‚©ãƒ«ãƒ€éšå±¤ã‚’å–å¾—
            folder_parts = self.folder_path.split('/')  # ãƒ‘ã‚¹ã‚’åˆ†å‰²
            if not folder_parts:
                logger.warning("âš ï¸ ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ãŒç„¡åŠ¹ã§ã™")  # ç„¡åŠ¹ãªãƒ‘ã‚¹è­¦å‘Š
                return []

            # ãƒ«ãƒ¼ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã‚’æ¤œç´¢
            root_folder_name = folder_parts[0]  # ãƒ«ãƒ¼ãƒˆãƒ•ã‚©ãƒ«ãƒ€å
            folder_query = f"name='{root_folder_name}' and mimeType='application/vnd.google-apps.folder'"
            folder_results = self.drive_service.files().list(
                q=folder_query,
                fields="files(id, name)"
            ).execute()

            folders = folder_results.get('files', [])
            if not folders:
                logger.warning(f"âš ï¸ Google Driveä¸Šã« '{root_folder_name}' ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")  # ãƒ•ã‚©ãƒ«ãƒ€æœªç™ºè¦‹è­¦å‘Š
                return []

            folder_id = folders[0]['id']  # ãƒ•ã‚©ãƒ«ãƒ€IDå–å¾—
            logger.info(f"ğŸ“‚ ãƒ•ã‚©ãƒ«ãƒ€ã‚’ç™ºè¦‹: {folders[0]['name']}")  # ãƒ•ã‚©ãƒ«ãƒ€ç™ºè¦‹ãƒ­ã‚°

            # æ®‹ã‚Šã®ãƒ•ã‚©ãƒ«ãƒ€éšå±¤ã‚’é †ã«æ¢ç´¢
            current_folder_id = folder_id
            for folder_name in folder_parts[1:]:
                subfolder_query = f"'{current_folder_id}' in parents and name='{folder_name}' and mimeType='application/vnd.google-apps.folder'"
                subfolder_results = self.drive_service.files().list(
                    q=subfolder_query,
                    fields="files(id, name)"
                ).execute()

                subfolders = subfolder_results.get('files', [])
                if not subfolders:
                    logger.warning(f"âš ï¸ ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ '{folder_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")  # ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€æœªç™ºè¦‹è­¦å‘Š
                    return []

                current_folder_id = subfolders[0]['id']  # æ¬¡ã®ãƒ•ã‚©ãƒ«ãƒ€ID
                logger.info(f"ğŸ“‚ ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã‚’ç™ºè¦‹: {folder_name}")  # ãƒ•ã‚©ãƒ«ãƒ€ç™ºè¦‹ãƒ­ã‚°

            # æœ€çµ‚çš„ãªãƒ•ã‚©ãƒ«ãƒ€IDã‚’ä¿å­˜
            csv_folder_id = current_folder_id

            # csvãƒ•ã‚©ãƒ«ãƒ€å†…ã®ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆãƒãƒ£ãƒ³ãƒãƒ«åãƒ•ã‚©ãƒ«ãƒ€ï¼‰ã‚’æ¤œç´¢
            full_path = '/'.join(folder_parts)  # å®Œå…¨ãªãƒ‘ã‚¹ã‚’æ§‹ç¯‰
            logger.info(f"ğŸ“‚ ç¾åœ¨ã®ãƒ•ã‚©ãƒ«ãƒ€: {full_path}")  # ç¾åœ¨ä½ç½®ãƒ­ã‚°
            logger.info(f"ğŸ” VCãƒãƒ£ãƒ³ãƒãƒ«ãƒ•ã‚©ãƒ«ãƒ€ã‚’æ¤œç´¢ä¸­...")  # ãƒãƒ£ãƒ³ãƒãƒ«ãƒ•ã‚©ãƒ«ãƒ€æ¤œç´¢ãƒ­ã‚°
            channel_folder_query = f"'{csv_folder_id}' in parents and mimeType='application/vnd.google-apps.folder'"
            channel_folder_results = self.drive_service.files().list(
                q=channel_folder_query,
                fields="files(id, name)"
            ).execute()

            channel_folders = channel_folder_results.get('files', [])
            logger.info(f"ğŸ“ {len(channel_folders)}å€‹ã®VCãƒãƒ£ãƒ³ãƒãƒ«ãƒ•ã‚©ãƒ«ãƒ€ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ")  # ãƒãƒ£ãƒ³ãƒãƒ«ãƒ•ã‚©ãƒ«ãƒ€æ•°ãƒ­ã‚°
            if channel_folders:
                logger.info(f"ğŸ“ ç™ºè¦‹ã—ãŸãƒãƒ£ãƒ³ãƒãƒ«ãƒ•ã‚©ãƒ«ãƒ€: {', '.join([f['name'] for f in channel_folders])}")  # ãƒãƒ£ãƒ³ãƒãƒ«åä¸€è¦§

            csv_files = []
            # å„ãƒãƒ£ãƒ³ãƒãƒ«ãƒ•ã‚©ãƒ«ãƒ€å†…ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            for channel_folder in channel_folders:
                channel_folder_id = channel_folder['id']
                channel_name = channel_folder['name']
                search_path = f"{full_path}/{channel_name}"  # æ¤œç´¢ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
                logger.debug(f"ğŸ” CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ä¸­: {search_path}")  # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°

                csv_query = f"'{channel_folder_id}' in parents and name contains '.csv'"
                csv_results = self.drive_service.files().list(
                    q=csv_query,
                    fields="files(id, name)"
                ).execute()

                channel_csv_files = csv_results.get('files', [])
                csv_files.extend(channel_csv_files)
                if channel_csv_files:
                    for csv_file in channel_csv_files:
                        logger.debug(f"  âœ… ç™ºè¦‹: {search_path}/{csv_file['name']}")  # CSVãƒ•ã‚¡ã‚¤ãƒ«åè¡¨ç¤º
                else:
                    logger.debug(f"  âš ï¸ {search_path}å†…ã«CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")  # CSVãƒ•ã‚¡ã‚¤ãƒ«ãªã—ãƒ­ã‚°
            logger.info(f"ğŸ“ åˆè¨ˆ{len(csv_files)}å€‹ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ")  # CSVãƒ•ã‚¡ã‚¤ãƒ«æ•°ãƒ­ã‚°

            return csv_files

        except HttpError as e:
            logger.error(f"âš ï¸ CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")  # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
            return []

    def read_csv_content(self, file_id: str, file_name: str) -> List[Dict[str, str]]:
        """
        CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’èª­ã¿è¾¼ã¿

        Args:
            file_id: ãƒ•ã‚¡ã‚¤ãƒ«ID
            file_name: ãƒ•ã‚¡ã‚¤ãƒ«å

        Returns:
            CSVãƒ¬ã‚³ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
        """
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            request = self.drive_service.files().get_media(fileId=file_id)
            file_content = io.BytesIO()
            downloader = MediaIoBaseDownload(file_content, request)

            done = False
            while not done:
                status, done = downloader.next_chunk()

            # CSVã‚’ãƒ‘ãƒ¼ã‚¹
            file_content.seek(0)
            csv_text = file_content.read().decode('utf-8')

            if not csv_text:
                logger.warning(f"âš ï¸ CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™: {file_name}")  # ç©ºãƒ•ã‚¡ã‚¤ãƒ«è­¦å‘Š
                return []

            lines = csv_text.strip().split('\n')
            if len(lines) < 2:  # ãƒ˜ãƒƒãƒ€ãƒ¼ã®ã¿ã®å ´åˆ
                return []

            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’å–å¾—
            headers = lines[0].split(',')

            # ãƒ‡ãƒ¼ã‚¿è¡Œã‚’ãƒ‘ãƒ¼ã‚¹
            records = []
            target_date_str = self.target_date.strftime('%Y/%m/%d')  # å¯¾è±¡æ—¥ä»˜æ–‡å­—åˆ—

            for line in lines[1:]:
                values = line.split(',')
                if len(values) != len(headers):
                    continue

                record = dict(zip(headers, values))

                # å¯¾è±¡æ—¥ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã¿æŠ½å‡º
                if 'datetime_jst' in record and record['datetime_jst'].startswith(target_date_str):
                    # VCãƒãƒ£ãƒ³ãƒãƒ«åã‚’è¿½åŠ ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ‹¡å¼µå­ã‚’é™¤ã„ãŸã‚‚ã®ï¼‰
                    record['vc_name'] = file_name.replace('.csv', '')
                    records.append(record)

            logger.info(f"ğŸ“– {file_name}ã‹ã‚‰{target_date_str}ã®{len(records)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")  # èª­ã¿è¾¼ã¿çµæœãƒ­ã‚°
            return records

        except Exception as e:
            logger.error(f"âš ï¸ CSVãƒ•ã‚¡ã‚¤ãƒ« {file_name} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")  # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
            return []

    def aggregate_user_data(self, all_records: List[Dict[str, str]]) -> Dict[str, Dict[str, Any]]:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„

        Args:
            all_records: å…¨CSVãƒ¬ã‚³ãƒ¼ãƒ‰

        Returns:
            ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®é›†è¨ˆãƒ‡ãƒ¼ã‚¿
        """
        user_data = defaultdict(lambda: {
            'user_name': '',
            'vc_channels': set(),
            'login_count': 0
        })

        for record in all_records:
            if record.get('present', '').upper() == 'TRUE':  # ãƒ­ã‚°ã‚¤ãƒ³ä¸­ã®å ´åˆ
                user_id = record.get('user_id', '')
                if user_id:
                    user_data[user_id]['user_name'] = record.get('user_name', '')  # ãƒ¦ãƒ¼ã‚¶ãƒ¼å
                    user_data[user_id]['vc_channels'].add(record.get('vc_name', ''))  # VCãƒãƒ£ãƒ³ãƒãƒ«è¿½åŠ 
                    user_data[user_id]['login_count'] += 1  # ãƒ­ã‚°ã‚¤ãƒ³å›æ•°ã‚«ã‚¦ãƒ³ãƒˆ

        # ã‚»ãƒƒãƒˆã‚’æ–‡å­—åˆ—ã«å¤‰æ›
        for user_id, data in user_data.items():
            data['vc_channels'] = ', '.join(sorted(data['vc_channels']))

        logger.info(f"ğŸ“ˆ {len(user_data)}åã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’é›†è¨ˆã—ã¾ã—ãŸ")  # é›†è¨ˆçµæœãƒ­ã‚°
        return dict(user_data)

    def get_sheet_id(self) -> Optional[str]:
        """Google Sheetsã®IDã‚’å–å¾—"""
        try:
            # Sheetsåã§æ¤œç´¢
            query = f"name='{self.sheet_name}' and mimeType='application/vnd.google-apps.spreadsheet'"
            results = self.drive_service.files().list(
                q=query,
                fields="files(id, name)"
            ).execute()

            sheets = results.get('files', [])
            if not sheets:
                logger.error(f"âš ï¸ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.sheet_name}")  # ã‚·ãƒ¼ãƒˆæœªç™ºè¦‹ã‚¨ãƒ©ãƒ¼
                return None

            sheet_id = sheets[0]['id']
            logger.info(f"ğŸ“Š ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’ç™ºè¦‹: {self.sheet_name}")  # ã‚·ãƒ¼ãƒˆç™ºè¦‹ãƒ­ã‚°
            return sheet_id

        except Exception as e:
            logger.error(f"âš ï¸ ã‚·ãƒ¼ãƒˆIDã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")  # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
            return None

    def ensure_sheets_exist(self, sheet_id: str):
        """å¿…è¦ãªã‚·ãƒ¼ãƒˆãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã€ãªã‘ã‚Œã°ä½œæˆ"""
        try:
            # æ—¢å­˜ã®ã‚·ãƒ¼ãƒˆä¸€è¦§ã‚’å–å¾—
            spreadsheet = self.sheets_service.spreadsheets().get(
                spreadsheetId=sheet_id
            ).execute()

            existing_sheets = [sheet['properties']['title'] for sheet in spreadsheet['sheets']]

            # å¿…è¦ãªã‚·ãƒ¼ãƒˆå
            required_sheets = ['daily_summary', 'user_statistics']

            # ä¸è¶³ã—ã¦ã„ã‚‹ã‚·ãƒ¼ãƒˆã‚’ä½œæˆ
            requests = []
            for sheet_name in required_sheets:
                if sheet_name not in existing_sheets:
                    requests.append({
                        'addSheet': {
                            'properties': {
                                'title': sheet_name
                            }
                        }
                    })
                    logger.info(f"ğŸ“„ æ–°ã—ã„ã‚·ãƒ¼ãƒˆã‚’ä½œæˆä¸­: {sheet_name}")  # ã‚·ãƒ¼ãƒˆä½œæˆãƒ­ã‚°

            if requests:
                self.sheets_service.spreadsheets().batchUpdate(
                    spreadsheetId=sheet_id,
                    body={'requests': requests}
                ).execute()

                # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¨­å®š
                self._set_sheet_headers(sheet_id)

        except Exception as e:
            logger.error(f"âš ï¸ ã‚·ãƒ¼ãƒˆã®ç¢ºèªãƒ»ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")  # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
            raise

    def _set_sheet_headers(self, sheet_id: str):
        """ã‚·ãƒ¼ãƒˆã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¨­å®š"""
        try:
            # daily_summaryã®ãƒ˜ãƒƒãƒ€ãƒ¼
            daily_headers = [['date', 'user_id', 'user_name', 'vc_channels', 'login_count']]
            self.sheets_service.spreadsheets().values().update(
                spreadsheetId=sheet_id,
                range='daily_summary!A1:E1',
                valueInputOption='RAW',
                body={'values': daily_headers}
            ).execute()

            # user_statisticsã®ãƒ˜ãƒƒãƒ€ãƒ¼
            stats_headers = [['user_id', 'user_name', 'last_login_date', 'consecutive_days',
                             'monthly_days', 'total_days', 'last_updated']]
            self.sheets_service.spreadsheets().values().update(
                spreadsheetId=sheet_id,
                range='user_statistics!A1:G1',
                valueInputOption='RAW',
                body={'values': stats_headers}
            ).execute()

            logger.info("âœ… ã‚·ãƒ¼ãƒˆã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¨­å®šã—ã¾ã—ãŸ")  # ãƒ˜ãƒƒãƒ€ãƒ¼è¨­å®šæˆåŠŸãƒ­ã‚°

        except Exception as e:
            logger.error(f"âš ï¸ ãƒ˜ãƒƒãƒ€ãƒ¼ã®è¨­å®šã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")  # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°

    def write_daily_summary(self, sheet_id: str, user_data: Dict[str, Dict[str, Any]]):
        """æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ã‚’ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿"""
        try:
            # æ›¸ãè¾¼ã‚€ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            rows = []
            date_str = self.target_date.strftime('%Y/%m/%d')

            for user_id, data in user_data.items():
                rows.append([
                    date_str,
                    user_id,
                    data['user_name'],
                    data['vc_channels'],
                    data['login_count']
                ])

            if not rows:
                logger.info("ğŸ“ daily_summaryã«æ›¸ãè¾¼ã‚€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")  # ãƒ‡ãƒ¼ã‚¿ãªã—ãƒ­ã‚°
                return

            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®æœ€çµ‚è¡Œã‚’å–å¾—
            result = self.sheets_service.spreadsheets().values().get(
                spreadsheetId=sheet_id,
                range='daily_summary!A:A'
            ).execute()

            existing_rows = result.get('values', [])
            next_row = len(existing_rows) + 1

            # ãƒ‡ãƒ¼ã‚¿ã‚’è¿½è¨˜
            range_name = f'daily_summary!A{next_row}:E{next_row + len(rows) - 1}'
            self.sheets_service.spreadsheets().values().update(
                spreadsheetId=sheet_id,
                range=range_name,
                valueInputOption='RAW',
                body={'values': rows}
            ).execute()

            logger.info(f"âœ… daily_summaryã‚·ãƒ¼ãƒˆã«{len(rows)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ›¸ãè¾¼ã¿ã¾ã—ãŸ")  # æ›¸ãè¾¼ã¿æˆåŠŸãƒ­ã‚°

        except Exception as e:
            logger.error(f"âš ï¸ daily_summaryã®æ›¸ãè¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")  # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°

    def update_user_statistics(self, sheet_id: str, user_data: Dict[str, Dict[str, Any]]):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼çµ±è¨ˆæƒ…å ±ã‚’æ›´æ–°"""
        try:
            # æ—¢å­˜ã®çµ±è¨ˆæƒ…å ±ã‚’èª­ã¿è¾¼ã¿
            result = self.sheets_service.spreadsheets().values().get(
                spreadsheetId=sheet_id,
                range='user_statistics!A:G'
            ).execute()

            existing_data = result.get('values', [])

            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’ã‚­ãƒ¼ã«ã—ãŸè¾æ›¸ã‚’ä½œæˆ
            stats_dict = {}
            for row in existing_data[1:] if existing_data else []:
                if len(row) >= 7:
                    stats_dict[row[0]] = {
                        'user_name': row[1],
                        'last_login_date': row[2],
                        'consecutive_days': int(row[3]) if row[3] else 0,
                        'monthly_days': int(row[4]) if row[4] else 0,
                        'total_days': int(row[5]) if row[5] else 0,
                        'last_updated': row[6]
                    }

            # çµ±è¨ˆæƒ…å ±ã‚’æ›´æ–°
            today_str = self.target_date.strftime('%Y/%m/%d')
            yesterday = self.target_date - timedelta(days=1)
            yesterday_str = yesterday.strftime('%Y/%m/%d')
            current_month = self.target_date.month

            for user_id in user_data:
                if user_id in stats_dict:
                    # æ—¢å­˜ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ›´æ–°
                    stats = stats_dict[user_id]

                    # é€£ç¶šãƒ­ã‚°ã‚¤ãƒ³æ—¥æ•°ã®è¨ˆç®—
                    if stats['last_login_date'] == yesterday_str:
                        stats['consecutive_days'] += 1  # å‰æ—¥ã‚‚ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ã„ãŸ
                    else:
                        stats['consecutive_days'] = 1  # é€£ç¶šãŒé€”åˆ‡ã‚ŒãŸ

                    # æœˆé–“ãƒ­ã‚°ã‚¤ãƒ³æ—¥æ•°ã®è¨ˆç®—
                    last_login_date = datetime.strptime(stats['last_login_date'], '%Y/%m/%d').date()
                    if last_login_date.month == current_month:
                        stats['monthly_days'] += 1  # åŒã˜æœˆ
                    else:
                        stats['monthly_days'] = 1  # æœˆãŒå¤‰ã‚ã£ãŸ

                    # ç´¯è¨ˆãƒ­ã‚°ã‚¤ãƒ³æ—¥æ•°
                    stats['total_days'] += 1

                    # æœ€çµ‚ãƒ­ã‚°ã‚¤ãƒ³æ—¥ã¨æ›´æ–°æ—¥æ™‚ã‚’æ›´æ–°
                    stats['last_login_date'] = today_str
                    stats['last_updated'] = datetime.now().strftime('%Y/%m/%d %H:%M:%S')
                    stats['user_name'] = user_data[user_id]['user_name']  # ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚‚æ›´æ–°

                else:
                    # æ–°è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼
                    stats_dict[user_id] = {
                        'user_name': user_data[user_id]['user_name'],
                        'last_login_date': today_str,
                        'consecutive_days': 1,
                        'monthly_days': 1,
                        'total_days': 1,
                        'last_updated': datetime.now().strftime('%Y/%m/%d %H:%M:%S')
                    }

            # ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã‚€ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            rows = [['user_id', 'user_name', 'last_login_date', 'consecutive_days',
                    'monthly_days', 'total_days', 'last_updated']]  # ãƒ˜ãƒƒãƒ€ãƒ¼

            for user_id, stats in sorted(stats_dict.items()):
                rows.append([
                    user_id,
                    stats['user_name'],
                    stats['last_login_date'],
                    stats['consecutive_days'],
                    stats['monthly_days'],
                    stats['total_days'],
                    stats['last_updated']
                ])

            # ã‚·ãƒ¼ãƒˆå…¨ä½“ã‚’æ›´æ–°
            self.sheets_service.spreadsheets().values().update(
                spreadsheetId=sheet_id,
                range='user_statistics!A:G',
                valueInputOption='RAW',
                body={'values': rows}
            ).execute()

            logger.info(f"âœ… {len(stats_dict)}åã®ãƒ¦ãƒ¼ã‚¶ãƒ¼çµ±è¨ˆæƒ…å ±ã‚’æ›´æ–°ã—ã¾ã—ãŸ")  # æ›´æ–°æˆåŠŸãƒ­ã‚°

        except Exception as e:
            logger.error(f"âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼çµ±è¨ˆæƒ…å ±ã®æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")  # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°

    def run(self):
        """é›†è¨ˆå‡¦ç†ã®ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
        try:
            logger.info(f"ğŸš€ {self.target_date}ã®ãƒ‡ãƒ¼ã‚¿é›†è¨ˆã‚’é–‹å§‹ã—ã¾ã™")  # é–‹å§‹ãƒ­ã‚°

            # 1. CSVãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
            csv_files = self.get_csv_files_from_drive()
            if not csv_files:
                logger.warning("âš ï¸ CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")  # CSVãƒ•ã‚¡ã‚¤ãƒ«ãªã—è­¦å‘Š
                return

            # 2. å„CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            all_records = []
            for csv_file in csv_files:
                records = self.read_csv_content(csv_file['id'], csv_file['name'])
                all_records.extend(records)

            logger.info(f"ğŸ“– åˆè¨ˆ{len(all_records)}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")  # ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ãƒ­ã‚°

            # 3. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„
            user_data = self.aggregate_user_data(all_records)

            if not user_data:
                logger.info("ğŸ“ˆ é›†è¨ˆã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")  # é›†ç´„ãƒ‡ãƒ¼ã‚¿ãªã—ãƒ­ã‚°
                return

            # 4. Google Sheetsã®IDã‚’å–å¾—
            sheet_id = self.get_sheet_id()
            if not sheet_id:
                logger.error("âš ï¸ ã‚·ãƒ¼ãƒˆIDãŒå–å¾—ã§ããªã„ãŸã‚ã€å‡¦ç†ã‚’ç¶šè¡Œã§ãã¾ã›ã‚“")  # ã‚·ãƒ¼ãƒˆIDå–å¾—å¤±æ•—ã‚¨ãƒ©ãƒ¼
                return

            # 5. å¿…è¦ãªã‚·ãƒ¼ãƒˆã‚’ç¢ºèªãƒ»ä½œæˆ
            self.ensure_sheets_exist(sheet_id)

            # 6. daily_summaryã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿
            self.write_daily_summary(sheet_id, user_data)

            # 7. user_statisticsã‚·ãƒ¼ãƒˆã‚’æ›´æ–°
            self.update_user_statistics(sheet_id, user_data)

            logger.info("ğŸ‰ ãƒ‡ãƒ¼ã‚¿é›†è¨ˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")  # å®Œäº†ãƒ­ã‚°

        except Exception as e:
            logger.error(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿é›†è¨ˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")  # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
            raise


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®ãƒ‘ãƒ¼ã‚¹
    parser = argparse.ArgumentParser(description='æ—¥æ¬¡VCãƒ­ã‚°ã‚¤ãƒ³é›†è¨ˆãƒ—ãƒ­ã‚°ãƒ©ãƒ ')
    parser.add_argument('--date', type=str, help='é›†è¨ˆå¯¾è±¡æ—¥ (YYYY-MM-DDå½¢å¼)')
    parser.add_argument('--debug', action='store_true', help='ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã‚’æœ‰åŠ¹åŒ–')
    parser.add_argument('--env', type=int, default=2, choices=[0, 1, 2],
                       help='å®Ÿè¡Œç’°å¢ƒ (0=æœ¬ç•ª, 1=ãƒ†ã‚¹ãƒˆ, 2=é–‹ç™º, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ=2)')  # ç’°å¢ƒå¼•æ•°è¿½åŠ 

    args = parser.parse_args()

    # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®è¨­å®š
    if args.debug:
        logger.remove()  # æ—¢å­˜ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’å‰Šé™¤
        logger.add(sys.stderr, level="DEBUG", format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}")  # DEBUGãƒ¬ãƒ™ãƒ«ã§ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
        logger.add("daily_aggregator.log", rotation="10 MB", retention="7 days", level="DEBUG", encoding="utf-8")  # DEBUGãƒ¬ãƒ™ãƒ«ã§ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›

    # å¯¾è±¡æ—¥ä»˜ã®è¨­å®š
    target_date = None
    if args.date:
        try:
            target_date = datetime.strptime(args.date, '%Y-%m-%d').date()
        except ValueError:
            logger.error(f"âš ï¸ æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒç„¡åŠ¹ã§ã™: {args.date}ã€‚YYYY-MM-DDå½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„")  # æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚¨ãƒ©ãƒ¼
            sys.exit(1)

    # ç’°å¢ƒã®è¨­å®š
    env = Environment(args.env)  # ç’°å¢ƒã‚’è¨­å®š
    env_name = EnvConfig.get_environment_name(env)  # ç’°å¢ƒåå–å¾—
    logger.info(f"ğŸŒ {env_name}ã§å®Ÿè¡Œä¸­ã§ã™")  # ç’°å¢ƒãƒ­ã‚°å‡ºåŠ›

    # é›†è¨ˆå‡¦ç†ã‚’å®Ÿè¡Œ
    aggregator = DailyAggregator(target_date, env)
    aggregator.run()


if __name__ == '__main__':
    main()